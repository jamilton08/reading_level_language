---
title: "Final Project 607"
author: "Jonathan Cruz"
date: "2024-05-08"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
library(igraph)
library(networkD3)
```

## Introduction

The aim of this project is to accurately assess students' reading levels by leveraging a neural network trained through supervised learning. While numerous algorithms serve this purpose, their predictions often exhibit significant grade discrepancies. I propose that employing a supervised learning model coupled with effective natural language processing strategies can effectively address these challenges.

- Libraries : Seaborn, Keras, Tensorflow, Transformers, Pandas, Numpy and Others

## Motivation

I encountered a website housing various algorithms designed to determine the grade level of a given text. When I input a seventh-grade reading level question from ChatGPT, the closest algorithm estimated it at a twelfth-grade reading level.

- Algorithms Score : https://github.com/jamilton08/reading_level_language/blob/main/Readability%20Scoring%20System.pdf

- Reading Level Algorithms - https://readabilityformulas.com/readability-scoring-system.php




## Generated Dataset

I conducted an experiment with ChatGPT where I posed 20 questions that could be answered by any grade level. Following this, I requested ChatGPT to respond to each question at every grade level.

-ChatGPT Promts - https://chat.openai.com/share/4d93bfca-b1ba-4bfa-947e-fbe0f37e5c6f


  
## Dataset Sample
```{r echo=FALSE}
df <- read.csv("https://raw.githubusercontent.com/jamilton08/reading_level_language/main/grade_level_dataset%20-%20Sheet1.csv")

df[1:5,] |>
          kbl() |>
          kable_material_dark("hover")

```

## Average Word Response For Same Questions

```{r echo = FALSE}
df <- df |>
      pivot_longer(
    cols = starts_with("grade_"), 
    names_to = "grade", 
    values_to = "response"
  )

df |>
  mutate(count = lengths(gregexpr("\ ", response)) + 1  ) |>
  mutate(grade = str_remove(grade, "grade_"))|> 
  mutate(grade = as.numeric(grade)) |>
  group_by(grade) |>
  summarise(average_count = mean(count)) |>
  ggplot( aes(x = grade,  y = average_count , fill = grade)) +    geom_bar(stat="identity") + theme(legend.position="none")

```




## Pre Model Effort

- Tokenizing using huggingface's bert pretrained tokenizer
- One hot encode

```{r echo=FALSE}
one_hot_matrix <- diag(1, 3)

# Print the matrix
print(one_hot_matrix)
```
- Due to it being a supervised model we have train dataset and test dataset

## Netowork Sturcture
- input layer - input ids you get from tokenization 

- hidden layer - Max(0, x), Relu

- output layer - \[ \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \] 

## Network

```{r}
data <- tibble(
  from=c("input_1", "input_1", "input_1", "input_1", "be1", "be1", "be2", "be2", "be3", "be3", "be...", "be...", "be..."),
  to=c("be1", "be2", "be3", "be...", "outpu1", "output2", "output1", "output3", "output2", "output4", "output1", "output3", "output4")
)

# Plot
p <- simpleNetwork(data, height="200px", width="200px")
p
```

## Performance

Link to Project

- Reading Level Network : https://github.com/jamilton08/reading_level_language/blob/main/reading_level.ipynb

## Difficulty

While the model achieved an impressive 75 percent accuracy in predicting grade levels, it exhibited significant confusion particularly in high school reading levels. Upon further investigation, I discovered that Lexile standards indicate consistent reading levels for grades 9 through 12.

- Lexile Chart - https://www.scholastic.com/parents/books-and-reading/reading-resources/book-selection-tips/lexile-levels-made-easy.html

## Conclusion 

Undoubtedly, a neural network proves highly effective in predicting reading levels, surpassing existing algorithms in accuracy. With a substantially larger dataset, I am confident this model could make a significant contribution in real-world applications. Achieving a predictive accuracy of 75, it comfortably aligns with business expectations, typically ranging between 70 to 90, thus positioning it well for practical implementation. 


